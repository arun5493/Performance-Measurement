
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
\usepackage{tabu}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


\usepackage{mdwmath}
\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Performance Measurement of Personal Computer}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Anubhab Majumdar}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Email: amajumd@ncsu.edu\\
Unity ID: amajumd}
\and
\IEEEauthorblockN{Arun Jaganathan}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Email: ajagana@ncsu.edu\\
Unity ID: ajagana}
}



% make the title area
\maketitle


\begin{abstract}

In this report we describe a set of experiments performed to benchmark the performance of a typical personal computer. The benchmarking is not a performance estimation based on hardware specification - we have measured the different overheads levied by the OS and determined the hardware constrains to gain a true perspective of the system performance. The report details out the experiments and analyzes the results to draw conclusions about the performance of the system being tested. 

\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% IEEEtran, journal, \LaTeX, paper, template.
% \end{IEEEkeywords}


\IEEEpeerreviewmaketitle



% ------------------------------------------------------------------------------------------

\section{Introduction}
Specifications of a computer system does not always convey it's true performance. The hardware and operating system introduces various overheads which constrains performance. The measure of these overheads are important as they help benchmark the true performance of any system. This knowledge is important for developers as their application runs atop the operating system and heavily uses the services provided by it; thus any bottlenecks in the OS will translate into their applications and degrade performance. Also, the OS determines the baseline "responsiveness" the user expects from the system and applications should not be far from this baseline to ensure smooth customer experience. 

\subsection{Goals}
Our primary goal was to benchmark the CPU, OS services and memory in details and analyze the results to draw conclusions about their performance. The CPU and OS services experiments are designed and implemented by Anubhab Majumdar. Arun Jaganathan designed the experiments to test memory components and implementation was shared by both the authors. 

\subsection{Language}
We used trusty C language to design and implement the experiments. The code was compiled with \textbf{Apple LLVM version 7.3.0 (clang-703.0.31)} with no optimizations because we wanted the assembly code to be in order of our original program. 

\subsection{Duration}
We worked on this project for around 70 hours spanning over 3 weeks. This includes determining the deliverables, reading relevant research papers, designing the experiments, coding the experiments, data consolidation, analysis of the data and drafting this report.

% ------------------------------------------------------------------------------------------

\section{Machine Description}
We have tested one of our personal computer, a MacBook Air. Following are the details of the machine:
\begin{enumerate}

\item
\textbf{Model Name}: MacBook Air
\item
\textbf{Model Identifier}: MacBookAir7,2
\item
\textbf{Processor Name}: Intel Core i5
\item
\textbf{Processor Speed}:  1.6 GHz
\item
\textbf{Number of Processors}: 1
\item
\textbf{Total Number of Cores}:  2
\item
\textbf{L2 Cache}: 256 KB (per core)
\item
\textbf{L3 Cache}: 3 MB
\item
\textbf{Memory}: 8 GB
\item
\textbf{Memory Type}: DDR3
\item
\textbf{Memory Speed}:  1600 MHz
\item
\textbf{Memory bus speed}: 1066 MHz 
\item
\textbf{Link Speed}: 5.0 GT/s
\item
\textbf{Link Width}: x4
\item
\textbf{Storage}: 128 GB 
\item
\textbf{Medium Type}:  Solid State Drive
\item
\textbf{Operating System}: MacOS Sierra (Version 10.12)

\end{enumerate}  

% ------------------------------------------------------------------------------------------

\section{Experiments}

The experiments are divided into two broad categories:
\begin{itemize}
\item
CPU, scheduling and OS services experiments
\item
Memory experiments 
\end{itemize}
Each of these categories are aggregation of small experiments that measure various overheads associated with OS or constraints of hardware. The following subsections describes the methodology, presents the findings and draw inference about it.
\par Before we dive into explanation about the experiments, we would like to explain the units of measurement. We have measured the operations in term of \textbf{cycles} and \textbf{time}. CPU cycles were measured using the C function \textbf{rdtsc} (Read Time Stamp Counter) to read the CPU cycles before and after any operation and the difference is assumed as the number of cycles consumed by the operation. We should also mention that before using rdtsc, we have used the C function \textbf{cpuid} to prevent any out of order execution by CPU. Similarly, time was measured in microseconds by using the \textbf{gettimeofday} function before and after an operation and the difference is considered as the time taken to perform the task.   
\par Another important point is that before executing the experiments we have restricted number of active cores in our system from 4 to 1 (see Fig.\ref{fig:onecore}).
\par For some operations like process creation or context switch, we couldn't estimate a base hardware performance because it is difficult to make an educated guess about how much cycles fork() or pipe() is actually consuming. In these cases we have marked the hardware base performance as "Undefined".

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{OneCore.png}
\caption{Restricting use of multi-core}
\label{fig:onecore}
\end{figure}


% ------------------------------------------------------------------------------------------


\subsection{CPU, scheduling and OS services experiments}

The experiments are conducted to measure 5 key overheads. Their names and descriptions are listed the subsections below.

\subsubsection{Measurement Overhead}

We start our benchmarking experiments by measuring overhead to perform 2 basic tasks - reading from memory and looping through multiple iterations of a task.

\paragraph{Methodology}

For reading time, we allocated a character array of size 10,000 bytes and read them one by one. The experiment was to measure, in terms of cycles and time, the cost of reading these 10,000 characters. At first, the cost of cpuid and that of running the loop without reading any character is measured.
The program is executed 100 times separately using bash script and the readings are noted. Next, the file read operation of 10,000 characters from the array is performed. The time-stamp counter values are measured just before and just after read operation. The difference is noted in a file. This, again, is executed separately 100 times and the results are noted. The mean and median are calculated on the results and noted.
\par In a similar fashion, the cost is measured in units of time as well. We replaced rdtsc with gettimeofday function and calculated the difference of time measured in microseconds. The results are noted in sheet "Reading Time" in measurements.xls file. 

\paragraph{Results}

The RAM speed of the machine is 1600 MHz (as specified in System Information). 
Thus to perform 1 read operation it should take 0.6 ns. 
For reading 10,000 characters it should take 6.25 $\mu$s. 
We estimated it would take thrice the time. We considered page fault, cache miss, TLB miss for overhead and hardware pre-fetching as advantage.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & 6.25 $\mu$s \\ 
  \hline
  \textbf{Estimated Overhead} &  12 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & 18.75 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 21.65 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 3.047038635 \\ 
  \hline
\end{tabular}
\end{center}

From the above table we could see our predicted read time is quite close to measured time. The difference could be because of our estimate of page faults handling time.
We believe the results are quite accurate as we have measured, to the best of out ability, only the read time and subtracted the for loop time from the results.

% ------------------------------------------------------------------------------------------

\subsubsection{Loop Overhead}

\paragraph{Methodology}

In this experiment we had to "report the overhead of using a loop to measure many iterations of an operation".
This experiment is quite similar to the previous one. We wrote a code which contains a for-loop "looping" 10,000 times. We measured the cost, in terms of both cycle and time, twice - once by commenting the for-loop part and once by keeping the for-loop. No OS overhead will be incurred as no system call is made.

\paragraph{Results}

The CPU speed is 1.6 GHz, i.e., it takes 0.6 ns to run one CPU cycle. The assembly code replacing the for-loop had 4 instructions. We estimated one cycle is required for each instruction.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & 24 $\mu$s \\ 
  \hline
  \textbf{Estimated Overhead} &  No OS overhead \\ 
  \hline
  \textbf{Predicted Performance} & 24 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 24.33 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 2.065493313  \\ 
  \hline
\end{tabular}
\end{center}

From the above table we could see our predicted read time is quite close to measured time. 
We believe the results are quite accurate as we have measured, to the best of out ability, only the for-loop time and subtracted any extra time we measured along with the for-loop (like $\_$cpuid function cost).


% ------------------------------------------------------------------------------------------

\subsubsection{Procedure Call Overhead}

\paragraph{Methodology}

Next, we had to measure procedure call overhead. 8 different procedures needed to be tested, each with different number of arguments. The functions themselves do not do anything, they return as soon as they are called. The cost of calling such functions are measured one by one in cycles.  

\paragraph{Results}

The main estimation we had to make was about how much time it takes for the control switch from caller to callee and again come back to the caller function after callee returns. We predicted the function call itself will take 1 cycle and the return from function will take 1 cycle. 

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & 2 cycles \\ 
  \hline
  \textbf{Estimated Overhead} &  8 cycles \\ 
  \hline
  \textbf{Predicted Performance (0 argument)} & 10 cycles \\ 
  \hline
  \textbf{Measured Performance (0 argument)} & 14.33 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 119.4266145  \\ 
  \hline
  \textbf{Measured Performance (1 argument)} & 15.32 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 66.55330969  \\ 
  \hline
  \textbf{Measured Performance (2 argument)} & 1.13 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 35.80560731  \\ 
  \hline
  \textbf{Measured Performance (3 argument)} & 45.3 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 84.10785524  \\ 
  \hline
  \textbf{Measured Performance (4 argument)} & 31.9 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 91.47859495  \\ 
  \hline
  \textbf{Measured Performance (5 argument)} & 52.31 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 272.4169967  \\ 
  \hline
  \textbf{Measured Performance (6 argument)} & 68.3 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 329.6719367  \\ 
  \hline
  \textbf{Measured Performance (7 argument)} & 39.75 cycles \\ 
  \hline
  \textbf{Standard Deviation} & 64.24013897  \\ 
  \hline
\end{tabular}
\end{center}
       
From the above table we could see our predicted read time is quite close to measured time. 
We believe the results are quite accurate as we have measured, to the best of out ability, only the function call time and subtracted any extra time we measured along with the function call (like $\_$cpuid function cost).
We can observe from the table that with increase in the number of arguments, the cost associated with the function call increases, though not linearly. We attribute the uneven pattern in measured performance to optimizations the CPU may have performed.  
The additional cost of adding another argument is measured as 3.63 cycles. This is the mean of the difference in cost between functions with one argument more than the other.

% ------------------------------------------------------------------------------------------

\subsubsection{System Call Overhead}

\paragraph{Methodology}

The purpose of this experiment was to measure a minimalistic system call cost. We choose to test with the \textbf{time} system call. It is lightweight and the result is never cached. So it served out purpose perfectly.
The procedure of measuring cost is similar to the previous two. We measure cost by running the code twice, once commenting the system call and once letting it run.

\paragraph{Results}

The main estimate we had to make was the OS TRAP and OS TRAP handler time and it was difficult to estimate as we didn't had information about the TRAP handler code. We estimated it to be more than procedure call. For the procedure call we were getting results less than 1 $\mu$s. So we predicted the performance to be little more than 1 $\mu$s with OS overhead.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & Undetermined \\ 
  \hline
  \textbf{Estimated Overhead} &  1 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & Greater than 1 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 2.06 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 1.096366819  \\ 
  \hline
\end{tabular}
\end{center}

We believe the results are quite accurate as we have measured, to the best of out ability, only the system call time and subtracted any extra time we measured along with the system call (like $\_$cpuid function cost). Also since time system call is not cached, repeated execution of the program does not affect result.


% ------------------------------------------------------------------------------------------

\subsubsection{Task creation and running time}

\paragraph{Methodology}
 In this experiment, we measured the cost associated with process creation. We measured the time/time-stamp-counter before and after fork() system call. The difference will provide the task creation time. 
 For measuring the running time, we included a for loop of 100 cycles in the child process followed by exit(0). Also, we added wait(childId) in the parent before measuring the time again. This forces the parent to wait for the child to exit before measuring the time for the second time. Thus this time we are measuring the child process running time.

\paragraph{Results}

We predicted the process creation and running to be a costly affair as the entire code and data needs to be copied for child process creation. We couldn't come up with a logical estimate for the time it would take to create or run a process. What we could guess is fork() would be more costly that time() system call we used before as it is not a minimalistic system call.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & Undetermined \\ 
  \hline
  \textbf{Estimated Overhead} &  20 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & Greater than 20 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 153.41 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 60.707349  \\ 
  \hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & Undetermined \\ 
  \hline
  \textbf{Estimated Overhead} &  200 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & Greater than 200 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 522.41 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 1121.665014  \\ 
  \hline
\end{tabular}
\end{center}

The details of all the measurement is present in the Process creation and Process running tabs of measurement.xls file. There we can see that the measured performance sometimes varies between individual executions. We attribute this variation to the non-determinism surfacing from scheduler.


% ------------------------------------------------------------------------------------------

\subsubsection{Thread creation and running time}

\paragraph{Methodology}

This is very familiar to the last experiment except one difference - instead of creating and running a new process we need to create and run a new thread performing the same task as previous. 
The cost measurement process is exactly the same.

\paragraph{Results}

Thread is a lightweight process. Thus we estimated its cost to be much less than process.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & Undetermined \\ 
  \hline
  \textbf{Estimated Overhead} &  10 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & Greater than 10 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 21.34 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 2.877621773  \\ 
  \hline
\end{tabular}
\end{center}



\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & Undetermined \\ 
  \hline
  \textbf{Estimated Overhead} &  50 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & Greater than 50 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 25.5 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 10.69804098  \\ 
  \hline
\end{tabular}
\end{center}

We believe our measurements to be quite accurate as we were careful in subtracting any extra instructions within our time measuring functions.


% ------------------------------------------------------------------------------------------

\subsubsection{Context switch time}

\paragraph{Methodology}

In this experiment we measured the context switch time using the pipe system call.
First, we created a pipe in parent process and used fork() to create a child process. Inside the child process we closed the input side of pipe, measured the time/time-stamp-counter and sent that value through the pipe and exit. Meanwhile, in the parent process, we close off the input side of the pipe and read the value sent by the child process. Once we get the value, we immediately measure time/time-stamp-counter value again and subtract the one we received from this. 
Here, the parent process has to wait till the child process sends the value, thus eliminating the need to include wait(childID) call. Also we are measuring the final time in parent process but only after child process sends it's data. This ensures a context switch has taken place and we have measurements before and after it takes place.

\paragraph{Results}

We have been very careful in measuring the cost in this experiment. We have measured, individually, the time taken to send and receive message from pipe. We have subtracted this costs from our measurement to ensure we achieve correct results.
We also estimated software overhead to be more than thread running overhead but less than process running overhead.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & Undetermined \\ 
  \hline
  \textbf{Estimated Overhead} &  150 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & Greater than 150 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 161.83 $\mu$s \\ 
  \hline
  \textbf{Standard Deviation} & 72.20929947  \\ 
  \hline
\end{tabular}
\end{center}

% ------------------------------------------------------------------------------------------

\subsection{Memory experiments}

% ------------------------------------------------------------------------------------------

\subsubsection{RAM access time}

The machine we are testing contains L1, L2 and L3 cache. The size of the caches are already mentioned in the machine description part. However, during execution we are reducing number of active core to 1, thereby reducing the L2 and L3 cache to 128KB and 768KB respectively. 


\paragraph{Methodology}

We measured the "back-to-back" load latency of arrays ranging from 2KB to 2MB and plot the curve of log2(ARRAY$\_$SIZE) VS the load latency. We then divide the graph into 3 parts, using our knowledge of cache size, and try to estimate the latency time of each part - which provides us the latency of caches and memory. 
\par First, we allocate the required memory for the array. Next, we read each array element and assign unique values to them. This is done to make sure all array elements are in cache and all page faults are handled. This ensures that page fault handling time won't interfere with our back-to-back load latency measurement.
\par Now that our data is in cache, we start accessing them again. This time we are accessing them from caches of different level (depending on array size they would be spread across different caches). However, while accessing them to read their values, we also measure the time required to access them each. The individual access time of all the elements are added and written in an output file along with the log2 of array size. This data is used to plot the graph.

\paragraph{Results}

The graph (Fig. \ref{fig:latency}) is an exponentially increasing graph. The inference drawn by us are:

\begin{figure}[h]
\centering
\includegraphics[scale=0.2]{latency.png}
\caption{Latency curve}
\label{fig:latency}
\end{figure}


\begin{itemize}
\item
The graph is increasing because the array size is increasing which needs more time to read all its values
\item
The graph is not increasing linearly. What this means is that the time taken to access 2x elements is not double of accessing x elements, it is more than that. Infact, this difference increases in the border of cache sizes, clearly demarcating the fact that the latency between different levels of cache are quite prominent.
\end{itemize}




% ------------------------------------------------------------------------------------------

\subsubsection{RAM bandwidth}

Memory bandwidth is defined as the rate at which data can be read or written into the memory. The memory bandwidth is measured in bytes/second. But in modern RAMs, the read and write bandwidth are very high, that we have started measuring them in Gigabytes/seconds. For DDR3 RAMs, we can expect a transfer rate as high as 15 gigabytes/second with DDR3 RAMs, under ideal conditions. But as there are overheads to be considered, we can still expect a read rate of about 10 GB/s and write rate of about 1 GB/s for DDR3 RAM with 1600 Mhz.
\par The software overheads to be considered for measuring memory bandwidth would be the general error of about 200 microseconds (for an array of 8MB) and the latency in reading and writing. Since writing into the array takes more cycles, the write speed will be lower than the read speed. Estimating the overhead in terms of time is not that easy we are handling huge amount of data. Thus we are guessing the performance after considering the above mentioned overheads. The practical read bandwidth can be expected about 10GB/s against the theoretical 12.5 GB/s, as there will be always a small latency when accessing. Similarly, the write bandwidth can be expected to be about 8 Gb/s against the theoretical 12.5Gb/s.



\paragraph{Methodology}

We classify the memory bandwidth into two components - Read bandwidth and Write bandwidth. In general, the read bandwidth will be higher than the write bandwidth. We have written a program that creates a very big array (of size about 8MB). The write bandwidth is computed by finding the time taken in assigning the entire array (of size 8MB) with some random value. i.e., we start the timer before the assignment and end the timer after the values are assigned, and we find the time difference. Using this value, we can compute the write speed of our RAM. 
\par Similarly, for computing the read speed, we just use the same concept as above, and we try to read the value of the array elements individually and assign into a variable.



\paragraph{Results}
We have found the read and write transfer rates, i.e the read/write bandwidth of our RAM. As expected, the read speed is higher than the write speed (approximately twice).

Write speed is noted below:
\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & 12.5 GB/sec \\ 
  \hline
  \textbf{Predicted Performance} & 8 GB/s \\ 
  \hline
  \textbf{Measured Performance} & 1.36 GB/s \\ 
  \hline
\end{tabular}
\end{center}

The read speed is noted below:

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & 12.5 GB/s \\ 
  \hline
  \textbf{Predicted Performance} & 10 GB/s \\ 
  \hline
  \textbf{Measured Performance} & 3.11 GB/s \\ 
  \hline
\end{tabular}
\end{center}




% ------------------------------------------------------------------------------------------

\subsubsection{Page fault service time}

The page size was found to be 4096 Bytes. Page fault service time is the  time taken to allocate an entire page into the cache/ram, from the disk. Whenever a page fault occurs i.e., when the file is not present in the memory, there will be a flag raised and the data will be fetched from the disk into the memory. The software overheads to be considered for this would be the delay in fetching the data from the disk, and the speed of the disk also influence the time.




\paragraph{Methodology}


We create a character pointer and using the mmap() function, we are allocating memory equivalent to the size of the page to this pointer. We mentioned the start address and the size (4096Bytes) and we are finding the time taken to execute this function. The pointer resides in the memory and thus, it can be used to estimate the service time.
To find the time taken to access a single byte, we divide it by the size of the page.


\paragraph{Results}

After running the program for several times, the page fault service time comes to about 21 ms, which is more than what we guessed.

\begin{center}
\begin{tabular}{ |c|c| } 
  \hline
  \textbf{Base Hardware Performance} & 8 $\mu$s \\ 
  \hline
  \textbf{Estimated Overhead} &  5-7 $\mu$s \\ 
  \hline
  \textbf{Predicted Performance} & 13-15 $\mu$s \\ 
  \hline
  \textbf{Measured Performance} & 21 $\mu$s \\ 
  \hline
\end{tabular}
\end{center}

Therefore page fault service time for 4096 bytes is 21 $\mu$s, i.e., 5.1 ns for 1 byte. Comparing this to our read time experimental results, it takes 21.65 $\mu$s to read 10,000 characters from RAM. This averages to 2.1 ns per byte.

% ------------------------------------------------------------------------------------------
\clearpage
\onecolumn
\section{Conclusion}

The experiments provide results which validates many of our theoretical understandings of how an operating systems works in tandem with hardware and the close relation between them.
\par The results from all the experiments are summarized below:\\

\begin{center}
\begin{tabular} { |c|c|c|c|c| } 
  \hline
  \textbf{Operation} & \textbf{Base Hardware Performance} & \textbf{Estimated Overhead} & \textbf{Predicted Performance} & \textbf{Measured Performance} \\
  \hline
  Loop & 24 $\mu$s & NA & 24 $\mu$s & 24.33 $\mu$s \\
  \hline
  Read & 6.25 $\mu$s & 12 $\mu$s & 18.25 $\mu$s & 21.65 $\mu$s \\
  \hline
  Procedure call  & 2 cycles & 8 cycles & 10 cycles & 14.33 $\mu$s \\
  \hline
  System Call & Undetermined & 1 $\mu$s & $\textgreater$1 $\mu$s & 2.06 $\mu$s \\
  \hline
  Process Creation & Undetermined & 20 $\mu$s & $\textgreater$20 $\mu$s & 153.41 $\mu$s \\
  \hline
  Process running & Undetermined & 200 $\mu$s & $\textgreater$200 $\mu$s & 522.41 $\mu$s \\
  \hline
  Thread creation & Undetermined & 10 $\mu$s & $\textgreater$10 $\mu$s & 21.34 $\mu$s \\
  \hline
  Thread running & Undetermined & 50 $\mu$s & $\textgreater$50 $\mu$s & 25.5 $\mu$s \\
  \hline
  Context switch & Undetermined & 150 $\mu$s & $\textgreater$150 $\mu$s & 161.83 $\mu$s \\
  \hline
  RAM bandwidth (R) & 12.5 GB/s & 4 GB/s & 8 GB/s & 1.36 GB/s \\
  \hline
  RAM bandwidth (W) & 12.5 GB/s & 2 GB/s & 10 GB/s & 3.11 GB/s \\
  \hline
  Page fault & 8 $\mu$s & 5-7 $\mu$s & 13-15 $\mu$s & 21 $\mu$s \\
  \hline
  
\end{tabular}
\end{center}

For a more visual comparison, we have plotted the CPU and OS services experimental results in a graph. (See Fig. \ref{fig:graph1})

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{graph1.png}
\caption{Results of CPU experiments}
\label{fig:graph1}
\end{figure}

The graph clearly shows the contrast between cost associated with the different tasks.\\
Some noteworthy points from the doing the whole project:
\begin{itemize}

\item
Process creation and process running is costliest of the tasks

\item
Thread is lightweight process - its cost is much less than process

\item
Context switch is costly, but less than process creation or running

\item
Reading time from memory is higher compared to procedure calls 

\item
Procedure calls are cheaper, but their cost increases with increasing parameters

\item
System calls vary by cost; minimalist ones like time() takes less time where as fork() takes much longer to finish

\item
Memory read bandwidth is greater than read. Thus read takes less time than writing to memory

\item
Latency increases as level of cache increases. Reading from caches take much lesser time than reading from RAM.

\item
Page fault cost is large. Infact, we saw through experiment results that it is much costlier than RAM read time.

\end{itemize}



% ------------------------------------------------------------------------------------------






\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\clearpage

\begin{thebibliography}{1}

\bibitem{imbench}
L.~McVoy and C.~Staelin, \emph{lmbench: Portable Tools for Performance Analysis}, \hskip 1em plus
0.5em minus 0.4em\relax San Diego, California, January 1996

\bibitem{pagefault}
Page~Fault, \url{https://en.wikipedia.org/wiki/Page_fault}

\bibitem{mmap}
Understanding~mmap(), \url{http://pubs.opengroup.org/onlinepubs/009695399/functions/mmap.html}

\bibitem{bandwidth}
Memory~Bandwidth, \url{ https://en.wikipedia.org/wiki/Memory_bandwidth}

\bibitem{RAM}
Understanding~RAM, \url{http://www.bleepingcomputer.com/tutorials/identifying-and-upgrading-ram/}

\end{thebibliography}










% that's all folks
\end{document}


